{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field\n",
    "\n",
    "from torchtext import data\n",
    "from transformers import T5Tokenizer, T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "TRG = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(t5.config.to_dict()['d_model'], t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src, decoder_input_ids=trg)\n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(4):\n",
    "    new_model = T5Network().cuda()\n",
    "    new_model.load_state_dict(torch.load(f'model_{i+1}.pt'))\n",
    "    models.append(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, max_len = 50):\n",
    "    model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).cuda()\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "            \n",
    "    return trg_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTEXT = \"The COVID‑19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID‑19), caused by severe acute respiratory syndrome coronavirus 2 (SARS‑CoV‑2). The outbreak was first identified in December 2019 in Wuhan, China. The World Health Organization declared the outbreak a Public Health Emergency of International Concern on 30 January 2020 and a pandemic on 11 March. As of 23 August 2020, more than 23.1 million cases of COVID‑19 have been reported in more than 188 countries and territories, resulting in more than 802,000 deaths; more than 14.8 million people have recovered.\"\n",
    "#QUERY = \"how many people have died of covid-19 ?\"\n",
    "#QUERY = \"how many people have recovered from covid-19 ?\"\n",
    "#QUERY = \"where was the outbreak first identified ?\"\n",
    "#QUERY = \"when did who declare emergency ?\"\n",
    "\n",
    "#CONTEXT = \"Common symptoms include fever, cough, fatigue, shortness of breath, and loss of sense of smell. Complications may include pneumonia and acute respiratory distress syndrome. The time from exposure to onset of symptoms is typically around five days but may range from two to fourteen days. There are several vaccine candidates in development, although none have completed clinical trials to prove their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\"\n",
    "#QUERY = \"what are some symptoms ?\"\n",
    "#QUERY = \"when do symptoms start to appear ?\"\n",
    "#QUERY = \"are there any medications ?\"\n",
    "\n",
    "#CONTEXT = \"Coronaviruses constitute the subfamily Orthocoronavirinae, in the family Coronaviridae, order Nidovirales, and realm Riboviria. They are enveloped viruses with a positive-sense single-stranded RNA genome and a nucleocapsid of helical symmetry. The genome size of coronaviruses ranges from approximately 26 to 32 kilobases, one of the largest among RNA viruses. They have characteristic club-shaped spikes that project from their surface, which in electron micrographs create an image reminiscent of the solar corona, from which their name derives.\"\n",
    "#QUERY = \"what is genome size of coronavirus ?\"\n",
    "#QUERY = \"how do coronavirus look like ?\"\n",
    "#QUERY = \"what family does coronavirus belong to ?\"\n",
    "\n",
    "#CONTEXT = \"SARS was a relatively rare disease; at the end of the epidemic in June 2003, the incidence was 8,422 cases with a case fatality rate (CFR) of 11%. No cases of SARS-CoV have been reported worldwide since 2004.\"\n",
    "#QUERY = \"how many cases of sars ?\"\n",
    "#QUERY = \"what is the case fatality rate ?\"\n",
    "\n",
    "text = \"context : \" + CONTEXT.lower() + \" question : \" + QUERY.lower()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context : sars was a relatively rare disease; at the end of the epidemic in june 2003, the incidence was 8,422 cases with a case fatality rate (cfr) of 11%. no cases of sars-cov have been reported worldwide since 2004. question : what is the case fatality rate ?\n",
      "\n",
      "\n",
      "['▁context', '▁', ':', '▁', 's', 'ar', 's', '▁was', '▁', 'a', '▁relatively', '▁rare', '▁disease', ';', '▁at', '▁the', '▁end', '▁of', '▁the', '▁epidemic', '▁in', '▁', 'jun', 'e', '▁2003', ',', '▁the', '▁incidence', '▁was', '▁8,', '42', '2', '▁cases', '▁with', '▁', 'a', '▁case', '▁fatal', 'ity', '▁rate', '▁(', 'c', 'f', 'r', ')', '▁of', '▁1', '1%', '.', '▁no', '▁cases', '▁of', '▁', 's', 'ar', 's', '-', 'cov', '▁have', '▁been', '▁reported', '▁worldwide', '▁since', '▁2004', '.', '▁question', '▁', ':', '▁what', '▁is', '▁the', '▁case', '▁fatal', 'ity', '▁rate', '▁', '?']\n",
      "\n",
      "\n",
      "[2625, 3, 10, 3, 7, 291, 7, 47, 3, 9, 4352, 3400, 1994, 117, 44, 8, 414, 13, 8, 24878, 16, 3, 6959, 15, 3888, 6, 8, 20588, 47, 9478, 4165, 357, 1488, 28, 3, 9, 495, 12699, 485, 1080, 41, 75, 89, 52, 61, 13, 209, 4704, 5, 150, 1488, 13, 3, 7, 291, 7, 18, 16745, 43, 118, 2196, 4388, 437, 4406, 5, 822, 3, 10, 125, 19, 8, 495, 12699, 485, 1080, 3, 58]\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print('\\n')\n",
    "print(tokens)\n",
    "print('\\n')\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tokens1 = translate_sentence(tokenizer.convert_tokens_to_ids(tokens), SRC, TRG, models[0])\n",
    "pred_tokens2 = translate_sentence(tokenizer.convert_tokens_to_ids(tokens), SRC, TRG, models[1])\n",
    "pred_tokens3 = translate_sentence(tokenizer.convert_tokens_to_ids(tokens), SRC, TRG, models[2])\n",
    "pred_tokens4 = translate_sentence(tokenizer.convert_tokens_to_ids(tokens), SRC, TRG, models[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> ▁1 1% </s>\n",
      "<pad> ▁1 1% </s>\n",
      "<pad> ▁1 1% </s>\n",
      "<pad> ▁1 1% </s>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokenizer.convert_ids_to_tokens(pred_tokens1)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(pred_tokens2)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(pred_tokens3)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(pred_tokens4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
