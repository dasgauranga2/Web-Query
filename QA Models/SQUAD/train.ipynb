{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from torchtext import data\n",
    "from transformers import T5Tokenizer, T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', '▁world', '▁how', '▁are', '▁you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello world how are you?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8774, 296, 149, 33, 25, 58]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "TRG = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TabularDataset.splits(\n",
    "                path = '',\n",
    "                train = 'squad.csv',\n",
    "                format = 'csv',\n",
    "                fields = fields,\n",
    "                skip_header = True)\n",
    "\n",
    "train_data , valid_data = train_data[0].split(split_ratio=0.96,\n",
    "                                             random_state = random.seed(4321))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49003\n",
      "2042\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.examples))\n",
    "print(len(valid_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [2625, 3, 10, 3165, 19127, 3115, 888, 81, 57, 3214, 6, 7070, 6, 42, 1664, 5989, 41, 2338, 666, 6, 2072, 32, 7259, 137, 822, 3, 10, 19127, 888, 57, 3214, 6, 7070, 6, 11, 125, 1307, 58], 'trg': [5989]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁context', '▁', ':', '▁adult', '▁insects', '▁typically', '▁move', '▁about', '▁by', '▁walking', ',', '▁flying', ',', '▁or', '▁sometimes', '▁swimming', '▁(', 'see', '▁below', ',', '▁loc', 'o', 'motion', ').', '▁question', '▁', ':', '▁insects', '▁move', '▁by', '▁walking', ',', '▁flying', ',', '▁and', '▁what', '▁else', '?']\n",
      "['▁swimming']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[10000])['src'])\n",
    "\n",
    "print(tokens)\n",
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[10000])['trg'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "                                (train_data, valid_data), \n",
    "                                batch_size = BATCH_SIZE,\n",
    "                                device = device,\n",
    "                                sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(t5.config.to_dict()['d_model'], t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src, decoder_input_ids=trg)\n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 76,988,544 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1\tTRAIN LOSS : 2.68\tVALID LOSS : 0.95\tTIME : 1160.39\n",
      "\n",
      "EPOCH : 2\tTRAIN LOSS : 0.71\tVALID LOSS : 0.64\tTIME : 1502.37\n",
      "\n",
      "EPOCH : 3\tTRAIN LOSS : 0.40\tVALID LOSS : 0.56\tTIME : 1378.84\n",
      "\n",
      "EPOCH : 4\tTRAIN LOSS : 0.28\tVALID LOSS : 0.54\tTIME : 1206.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    # TRAIN \n",
    "    #########################################################################\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "    #########################################################################\n",
    "    \n",
    "    # VALID\n",
    "    #########################################################################\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    #########################################################################\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"EPOCH : {epoch+1}\\tTRAIN LOSS : {train_loss:.2f}\\tVALID LOSS : {valid_loss:.2f}\\tTIME : {end-start:.2f}\\n\")\n",
    "    torch.save(model.state_dict(), f'model_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "            \n",
    "    return trg_indexes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : ▁context ▁ : ▁ n a pole on ▁ i i i , ▁since ▁the ▁crime an ▁war ▁ b rita in ' s ▁closest ▁ ally , ▁visited ▁ l on don ▁in ▁ a pri l ▁18 55 , ▁and ▁from ▁17 ▁to ▁28 ▁august ▁the ▁same ▁year ▁ vic tori a ▁and ▁al bert ▁returned ▁the ▁visit . ▁question ▁ : ▁when ▁did ▁ n a pole on ▁ i i i ▁visit ▁ l on don ?\n",
      "TRG : ▁ a pri l ▁18 55\n",
      "PREDICTED : ▁ a pri l ▁18 55 </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁during ▁the ▁early ▁20 th ▁century ▁it ▁became ▁increasingly ▁common ▁to ▁ bury ▁crema ted ▁remains ▁rather ▁than ▁co ffin s ▁in ▁the ▁ab be y . ▁question ▁ : ▁in ▁the ▁early ▁20 th ▁century ▁it ▁became ▁more ▁common ▁to ▁ bury ▁what ▁kind ▁of ▁remains ?\n",
      "TRG : ▁crema ted\n",
      "PREDICTED : ▁crema ted </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁at ▁age ▁eight , ▁be y on c é ▁and ▁childhood ▁friend ▁ kel ly ▁row l and ▁met ▁la t avi a ▁ rob erson ▁while ▁in ▁an ▁audition ▁for ▁an ▁all - girl ▁entertainment ▁group . ▁question ▁ : ▁how ▁old ▁was ▁be y on c é ▁when ▁she ▁met ▁la t avi a ▁ rob erson ?\n",
      "TRG : ▁eight\n",
      "PREDICTED : ▁eight </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁most ▁elect r ification ▁systems ▁use ▁overhead ▁wire s , ▁but ▁third ▁rail ▁is ▁an ▁option ▁up ▁to ▁about ▁1, 200 ▁ v . ▁third ▁rail ▁systems ▁exclusively ▁use ▁ d c ▁distribution . ▁the ▁use ▁of ▁ a c ▁is ▁not ▁feasible ▁because ▁the ▁dimensions ▁of ▁ a ▁third ▁rail ▁are ▁physically ▁very ▁large ▁ compared ▁with ▁the ▁skin ▁depth ▁that ▁the ▁ alternating ▁current ▁penetrate s ▁to ▁(0 . ▁question ▁ : ▁what ▁depth ▁does ▁the ▁ alternating ▁current ▁penetrate ▁in ▁ a ▁steel ▁rail ?\n",
      "TRG : ▁skin ▁depth\n",
      "PREDICTED : ▁skin </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁un i code ▁defines ▁two ▁mapping ▁methods : ▁the ▁un i code ▁transformation ▁format ▁( u t f ) ▁ en coding s , ▁and ▁the ▁universal ▁code d ▁character ▁set ▁( u c s ) ▁ en coding s . ▁question ▁ : ▁what ▁are ▁the ▁two ▁mapping ▁methods ▁that ▁un i code ▁defines ?\n",
      "TRG : ▁un i code ▁transformation ▁format ▁( u t f ) ▁ en coding s , ▁and ▁the ▁universal ▁code d ▁character ▁set ▁( u c s ) ▁ en coding s\n",
      "PREDICTED : ▁un i code ▁transformation ▁format ▁( u t f ) ▁ en coding s , ▁and ▁the ▁universal ▁code d ▁character ▁set ▁( u c s ) ▁ en coding s </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁as ▁the ▁image ▁of ▁god ▁grows ▁within ▁man , ▁ he ▁learn s ▁to ▁ rely ▁less ▁on ▁an ▁intellectual ▁pursuit ▁of ▁virtue ▁and ▁more ▁on ▁an ▁affect ive ▁pursuit ▁of ▁charity ▁and ▁me e k ness . ▁me e k ness ▁and ▁charity ▁guide ▁ christ ians ▁to ▁acknowledge ▁that ▁they ▁are ▁nothing ▁without ▁the ▁one ▁( go d / christ ) ▁who ▁created ▁them , ▁sustain s ▁them , ▁and ▁guides ▁them . ▁question ▁ : ▁according ▁to ▁ christ i an ity , ▁who ▁is ▁the ▁\" one \" ?\n",
      "TRG : ▁god / christ\n",
      "PREDICTED : ▁go d / christ </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁in ▁1997 , ▁ omb ▁issued ▁ a ▁federal ▁register ▁notice ▁regarding ▁revision s ▁to ▁the ▁standards ▁for ▁the ▁classification ▁of ▁federal ▁data ▁on ▁race ▁and ▁ethnic ity . ▁question ▁ : ▁who ▁decided ▁on ▁the ▁standards ▁for ▁the ▁classification ▁of ▁race ▁and ▁ethnic ity ▁and ▁federal ▁data ?\n",
      "TRG : ▁ omb\n",
      "PREDICTED : ▁ omb </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁ ber mud a ▁has ▁developed ▁ a ▁proud ▁rugby ▁union ▁community . ▁the ▁ ber mud a ▁rugby ▁union ▁team ▁won ▁the ▁2011 ▁ca rib be an ▁championship s , ▁defeat ing ▁guy an a ▁in ▁the ▁final . ▁question ▁ : ▁who ▁won ▁the ▁2011 ▁ca rib be an ▁championship s ?\n",
      "TRG : ▁the ▁ ber mud a ▁rugby ▁union ▁team\n",
      "PREDICTED : ▁the ▁ ber mud a ▁rugby ▁union ▁team </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁in ▁15 66 , ▁the ▁first ▁police ▁investigator ▁of ▁ r i o ▁de ▁ jan eiro ▁was ▁recruited . ▁by ▁the ▁17 th ▁century , ▁most ▁captain cie s ▁already ▁had ▁local ▁units ▁with ▁law ▁enforcement ▁functions . ▁question ▁ : ▁how ▁had ▁the ▁ r i o ▁police ▁grown ▁by ▁the ▁17 th ▁century ?\n",
      "TRG : ▁most ▁captain cie s ▁already ▁had ▁local ▁units ▁with ▁law ▁enforcement ▁functions\n",
      "PREDICTED : ▁by ▁the ▁17 th ▁century , ▁most ▁captain cie s ▁already ▁had ▁local ▁units ▁with ▁law ▁enforcement ▁functions </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁before ▁prince ▁al bert ' s ▁death , ▁the ▁palace ▁was ▁frequently ▁the ▁scene ▁of ▁musical ▁entertainment s , ▁and ▁the ▁greatest ▁contemporary ▁musicians ▁entertained ▁at ▁ buck ingham ▁palace . ▁the ▁composer ▁fel ix ▁me ndel s s ohn ▁is ▁known ▁to ▁have ▁played ▁there ▁on ▁three ▁occasions . ▁question ▁ : ▁which ▁composer ▁played ▁at ▁ buck ingham ▁on ▁three ▁occasions ?\n",
      "TRG : ▁fel ix ▁me ndel s s ohn\n",
      "PREDICTED : ▁fel ix ▁me ndel s s ohn </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁some ▁alloy s ▁occur ▁naturally , ▁such ▁as ▁elect rum , ▁which ▁is ▁an ▁alloy ▁that ▁is ▁native ▁to ▁earth , ▁consist ing ▁of ▁silver ▁and ▁gold . ▁meteor ites ▁are ▁sometimes ▁made ▁of ▁naturally ▁occurring ▁alloy s ▁of ▁iron ▁and ▁nickel , ▁but ▁are ▁not ▁native ▁to ▁the ▁earth . ▁one ▁of ▁the ▁first ▁alloy s ▁made ▁by ▁humans ▁was ▁bronze , ▁which ▁is ▁made ▁by ▁mixing ▁the ▁metal s ▁ t in ▁and ▁copper . ▁question ▁ : ▁which ▁was ▁on ▁of ▁the ▁first ▁alloy s ▁made ▁by ▁humans ?\n",
      "TRG : ▁bronze\n",
      "PREDICTED : ▁bronze </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁from ▁circa ▁19 32 ▁until ▁1977 , ▁general ▁electric ▁poll ute d ▁the ▁ hou s at onic ▁river ▁with ▁ p c b s ▁discharge s ▁from ▁the ▁general ▁electric ▁plant ▁at ▁pit t s field , ▁mass ach use t t s . ▁question ▁ : ▁which ▁river ▁did ▁ge ▁poll ute ▁with ▁ p c b s ▁from ▁its ▁plant ▁in ▁pit t s field , ▁ma ?\n",
      "TRG : ▁ hou s at onic ▁river\n",
      "PREDICTED : ▁ hou s at onic ▁river </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁the ▁18 50 ▁census ▁saw ▁ a ▁dramatic ▁shift ▁in ▁the ▁way ▁information ▁about ▁residents ▁was ▁collected . ▁for ▁the ▁first ▁time , ▁free ▁persons ▁were ▁listed ▁individually ▁instead ▁of ▁by ▁head ▁of ▁household . ▁question ▁ : ▁how ▁were ▁free ▁persons ▁listed ▁in ▁the ▁us ▁census ▁prior ▁to ▁18 50 ?\n",
      "TRG : ▁by ▁head ▁of ▁household\n",
      "PREDICTED : ▁individually </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁groups ▁of ▁humans ▁have ▁always ▁identified ▁themselves ▁as ▁distinct ▁from ▁neighbor ing ▁groups , ▁but ▁such ▁differences ▁have ▁not ▁always ▁been ▁understood ▁to ▁be ▁natural , ▁im mut able ▁and ▁global . ▁question ▁ : ▁what ▁have ▁human ▁groups ▁always ▁considered ▁themselves ▁as ▁ compared ▁to ▁other ▁nearby ▁groups ?\n",
      "TRG : ▁distinct\n",
      "PREDICTED : ▁distinct ▁from ▁neighbor ing ▁groups </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁following ▁the ▁success ▁of ▁season ▁one , ▁the ▁second ▁season ▁was ▁moved ▁up ▁to ▁air ▁in ▁ jan u ary ▁2003 . ▁the ▁number ▁of ▁episodes ▁increased , ▁as ▁did ▁the ▁show ' s ▁budget ▁and ▁the ▁charge ▁for ▁commercial ▁spots . ▁dunkle man ▁left ▁the ▁show , ▁leaving ▁sea crest ▁as ▁the ▁ l one ▁host . ▁ k risti n ▁ a dam s ▁was ▁ a ▁correspondent ▁for ▁this ▁season . ▁question ▁ : ▁what ▁year ▁did ▁season ▁two ▁of ▁american ▁idol ▁first ▁air ?\n",
      "TRG : ▁2003\n",
      "PREDICTED : ▁2003 </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁the ▁rugby ▁union ▁team ▁the ▁rock ▁is ▁the ▁eastern ▁can a dian ▁entry ▁in ▁the ▁ america s ▁rugby ▁championship . ▁the ▁rock ▁play ▁their ▁home ▁games ▁at ▁ s w il ers ▁rugby ▁park , ▁as ▁did ▁the ▁rugby ▁canada ▁super ▁league ▁champion s ▁for ▁2005 ▁and ▁2006 , ▁the ▁new found l and ▁rock . ▁the ▁city ▁hosted ▁ a ▁rugby ▁world ▁cup ▁qualifying ▁match ▁between ▁canada ▁and ▁the ▁us a ▁on ▁12 ▁august ▁2006 , ▁where ▁the ▁can a dian s ▁heavily ▁defeated ▁the ▁us a ▁56 – 7 ▁to ▁qualify ▁for ▁the ▁2007 ▁rugby ▁world ▁cup ▁final s ▁in ▁fr ance . ▁question ▁ : ▁in ▁what ▁year ▁did ▁canada ▁beat ▁us a ▁56 – 7 ▁in ▁ a ▁rugby ▁world ▁cup ▁qualifying ▁match ?\n",
      "TRG : ▁2006\n",
      "PREDICTED : ▁2006 </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁another ▁family ▁of ▁bowl back ▁man d olin s ▁came ▁from ▁mi lan ▁and ▁ lomb ard y . ▁these ▁man d olin s ▁are ▁closer ▁to ▁the ▁man d olin o ▁or ▁man d or e ▁than ▁other ▁modern ▁man d olin s . ▁they ▁are ▁shorter ▁and ▁wider ▁than ▁the ▁standard ▁ n e a polita n ▁man d olin , ▁with ▁ a ▁shallow ▁back . ▁the ▁instruments ▁have ▁6 ▁strings , ▁3 ▁wire ▁ tre ble - string s ▁and ▁3 ▁gut ▁or ▁wire - w rap ped - s il k ▁bass - string s . ▁question ▁ : ▁how ▁many ▁strings ▁do ▁the ▁ lomb ard ic ▁man d olin s ▁have ?\n",
      "TRG : ▁6 ▁strings\n",
      "PREDICTED : ▁6 </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁starting ▁in ▁the ▁1970 s , ▁the ▁ bron x ▁often ▁symbolize d ▁violence , ▁decay , ▁and ▁urban ▁ruin . ▁the ▁wave ▁of ▁ar son ▁in ▁the ▁south ▁ bron x ▁in ▁the ▁1960 s ▁and ▁1970 s ▁inspired ▁the ▁observation ▁that ▁\" the ▁ bron x ▁is ▁burning \" : ▁in ▁1974 ▁it ▁was ▁the ▁title ▁of ▁both ▁ a ▁new ▁york ▁times ▁editorial ▁and ▁ a ▁ b b c ▁documentary ▁film . ▁question ▁ : ▁where ▁was ▁ar son ▁ a ▁big ▁problem ▁in ▁the ▁ bron x ?\n",
      "TRG : ▁the ▁south ▁ bron x\n",
      "PREDICTED : ▁south ▁ bron x </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁on ▁ jun e ▁14, ▁1987 , ▁about ▁ 5,000 ▁people ▁ gathered ▁again ▁at ▁freedom ▁monument ▁in ▁ rig a , ▁and ▁laid ▁flowers ▁to ▁commemorat e ▁the ▁anniversary ▁of ▁ stal in ' s ▁mass ▁de port ation ▁of ▁la t v ians ▁in ▁1941 . ▁this ▁was ▁the ▁first ▁large ▁demonstration ▁in ▁the ▁bal tic ▁republic s ▁to ▁commemorat e ▁the ▁anniversary ▁of ▁an ▁event ▁contrary ▁to ▁official ▁so vie t ▁history . ▁the ▁authorities ▁did ▁not ▁crack ▁down ▁on ▁demonstr ators , ▁which ▁encouraged ▁more ▁and ▁larger ▁demonstration s ▁throughout ▁the ▁bal tic ▁states . ▁question ▁ : ▁where ▁did ▁further ▁protest s ▁take ▁place ?\n",
      "TRG : ▁bal tic ▁states\n",
      "PREDICTED : ▁bal tic ▁states </s>\n",
      "\n",
      "SRC : ▁context ▁ : ▁shell ▁sold ▁9. 5% ▁of ▁its ▁23 . 1% ▁stake ▁in ▁wood side ▁petroleum ▁in ▁ jun e ▁2014 ▁and ▁advised ▁that ▁it ▁had ▁reached ▁an ▁agreement ▁for ▁wood side ▁to ▁buy ▁back ▁9. 5% ▁of ▁its ▁shares ▁at ▁ a ▁later ▁stage . ▁shell ▁became ▁ a ▁major ▁shareholder ▁in ▁wood side ▁after ▁ a ▁2001 ▁take over ▁attempt ▁was ▁blocked ▁by ▁then ▁federal ▁treasure r ▁pe ter ▁cost ello ▁and ▁the ▁corporation ▁has ▁been ▁open ▁about ▁its ▁intention ▁to ▁sell ▁its ▁stake ▁in ▁wood side ▁as ▁part ▁of ▁its ▁target ▁to ▁shed ▁assets . ▁question ▁ : ▁who ▁blocked ▁the ▁take over ▁attempt ?\n",
      "TRG : ▁then ▁federal ▁treasure r ▁pe ter ▁cost ello\n",
      "PREDICTED : ▁pe ter ▁cost ello </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0,len(valid_data.examples)),20)\n",
    "\n",
    "for i in idxs:\n",
    "    src = vars(valid_data.examples[i])['src']\n",
    "    trg = vars(valid_data.examples[i])['trg']\n",
    "    translation = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "    print(f\"SRC : {' '.join(tokenizer.convert_ids_to_tokens(src))}\")\n",
    "    print(f\"TRG : {' '.join(tokenizer.convert_ids_to_tokens(trg))}\")\n",
    "    print(f\"PREDICTED : {' '.join(tokenizer.convert_ids_to_tokens(translation))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
